{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "trainModel(Resnet50).ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cubic-scoop"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import Resize, ToTensor, Normalize"
      ],
      "id": "cubic-scoop",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "built-elevation"
      },
      "source": [
        "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
        "test_dir = '/opt/ml/input/data/eval'"
      ],
      "id": "built-elevation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quiet-organizer"
      },
      "source": [
        "## 미사용 to resnet"
      ],
      "id": "quiet-organizer"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acknowledged-easter"
      },
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self, num_classes: int = 1000):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(32, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "id": "acknowledged-easter",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3333eeb-380d-4299-b14d-5d9a20fbffc9"
      },
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self, num_classes=1000, init_weights=True):\n",
        "        super(VGG, self).__init__()\n",
        "        \n",
        "        #self.features = features #convolution\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 512, kernel_size=11, stride=4, padding=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(32, num_classes),\n",
        "        )#FC layer\n",
        "        \n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x) #Convolution \n",
        "        x = self.avgpool(x) # avgpool\n",
        "        x = x.view(x.size(0), -1) #\n",
        "        #x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x) #FC layer\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)"
      ],
      "id": "e3333eeb-380d-4299-b14d-5d9a20fbffc9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "extensive-north"
      },
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, img_paths, transform):\n",
        "        self.img_paths = img_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = Image.open(self.img_paths[index])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)"
      ],
      "id": "extensive-north",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39a38035-ee89-4ed6-a0a0-3a23e33f64f4"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data as data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "def file_load(opt):\n",
        "    \n",
        "    #data_path = []\n",
        "    #f = open(\"{0}.txt\".format(opt), 'r')\n",
        "    files = glob.glob(\"/opt/ml/input/data/train/images/*/*\") \n",
        "    #while True:\n",
        "        #line = f.readline()\n",
        "        #if not line: break\n",
        "        #data_path.append(line[:-1])\n",
        "    #f.close()\n",
        "    return files\n",
        "\n",
        "class CustomDataset(data.Dataset):\n",
        "    def __init__(self, opt_data):\n",
        "        super(CustomDataset, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        opt_data : 'train', 'validation'\n",
        "        \n",
        "        \"\"\"\n",
        "        #self.file_list = file_load('/opt/ml/input/data/train/train_path.csv')\n",
        "        y = pd.read_csv('/opt/ml/input/data/train/train_path.csv', index_col=0)\n",
        "        self.y = y.values\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        x = np.load(self.file_list[index])\n",
        "        self.x_data = torch.from_numpy(x).float()\n",
        "        self.y_data = torch.from_numpy(self.y[index]).float()\n",
        "        return self.x_data, self.y_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "        \n",
        "if __name__ == \"__main__\":\n",
        "    a = CustomDataset('train')"
      ],
      "id": "39a38035-ee89-4ed6-a0a0-3a23e33f64f4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyPas-44h1iC"
      },
      "source": [
        "## resnet"
      ],
      "id": "lyPas-44h1iC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "7ba8a5a0-8ebf-4249-a53e-d1dd2e5e38fc",
        "outputId": "84de5ae8-e048-47fe-934f-974561c616cf"
      },
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "model = ResNet50(include_top=True, weights=None, input_shape=(64, 48, 3), pooling=max, classes=18)"
      ],
      "id": "7ba8a5a0-8ebf-4249-a53e-d1dd2e5e38fc",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-08-26 06:16:22.767056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-26 06:16:22.768364: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2021-08-26 06:16:22.768436: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2021-08-26 06:16:22.768447: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2021-08-26 06:16:22.768801: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e457ffb2-97ef-4aef-a79d-66dbe1007717"
      },
      "source": [
        "def setup_gpus():\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            tf.config.experimental.set_visible_devices(gpus[0],'GPU')\n",
        "            tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1500)])\n",
        "        except RuntimeError as e:\n",
        "            print(e)"
      ],
      "id": "e457ffb2-97ef-4aef-a79d-66dbe1007717",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdc6f001-bf1a-42ad-b034-ae471794ae6a",
        "outputId": "b1ee9a76-d469-4dd4-a561-4ae1a0d459bc"
      },
      "source": [
        "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "print('compile end')\n"
      ],
      "id": "fdc6f001-bf1a-42ad-b034-ae471794ae6a",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compile end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "54cde698-4db7-4a23-8e94-2a6b2e32a5a1",
        "outputId": "5a7b66a7-6866-4dda-fa9b-7f1c71292daf"
      },
      "source": [
        "from PIL import Image\n",
        "from numpy import genfromtxt\n",
        "import gzip\n",
        "import _pickle\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio\n",
        "import cv2\n",
        "\n",
        "\n",
        "def dir_to_dataset(glob_files, loc_train_labels=\"\"):\n",
        "    print(\"Gonna process:\\n\\t %s\"%glob_files)\n",
        "    dataset = []\n",
        "    for file_count, file_name in enumerate( sorted(glob(glob_files)) ):\n",
        "        img = imageio.imread(file_name, pilmode='RGB')\n",
        "        img = cv2.resize(img, (48, 64))\n",
        "        #pixels = [f[0] for f in list(img.getdata())]\n",
        "        dataset.append(img)\n",
        "        if file_count % 1000 == 0:\n",
        "            print(\"\\t %s files processed\"%file_count)\n",
        "    print('done')\n",
        "    print(img.shape)\n",
        "        \n",
        "    # outfile = glob_files+\"out\"\n",
        "    # np.save(outfile, dataset)\n",
        "    if len(loc_train_labels) > 0:\n",
        "        df = pd.read_csv(loc_train_labels, names = [\"class\"])\n",
        "        return np.array(dataset), np.array(df[\"class\"])\n",
        "    else:\n",
        "        return np.array(dataset)\n",
        "    \n",
        "Data1, y1 = dir_to_dataset(\"/opt/ml/input/data/train/images/*/*\",\"/opt/ml/input/data/train/train_path.csv\")\n",
        "\n",
        "print('yup')\n",
        "\n",
        "# Data and labels are read \n",
        "train_num = 17000\n",
        "valid_num = 1000\n",
        "test_num = 900\n",
        "\n",
        "train_set_x = Data1[:train_num]\n",
        "train_set_y = y1[1:train_num+1]\n",
        "val_set_x = Data1[17000:17000+valid_num]\n",
        "val_set_y = y1[17001:17001+valid_num]\n",
        "test_set_x = Data1[18000:18000+test_num]\n",
        "test_set_y = y1[18001:18001+test_num]\n",
        "\n",
        "train_set = train_set_x, train_set_y\n",
        "val_set = val_set_x, val_set_y\n",
        "test_set = test_set_x, test_set_y\n",
        "\n",
        "dataset = [train_set, val_set, test_set]"
      ],
      "id": "54cde698-4db7-4a23-8e94-2a6b2e32a5a1",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gonna process:\n",
            "\t /opt/ml/input/data/train/images/*/*\n",
            "\t 0 files processed\n",
            "\t 1000 files processed\n",
            "\t 2000 files processed\n",
            "\t 3000 files processed\n",
            "\t 4000 files processed\n",
            "\t 5000 files processed\n",
            "\t 6000 files processed\n",
            "\t 7000 files processed\n",
            "\t 8000 files processed\n",
            "\t 9000 files processed\n",
            "\t 10000 files processed\n",
            "\t 11000 files processed\n",
            "\t 12000 files processed\n",
            "\t 13000 files processed\n",
            "\t 14000 files processed\n",
            "\t 15000 files processed\n",
            "\t 16000 files processed\n",
            "\t 17000 files processed\n",
            "\t 18000 files processed\n",
            "done\n",
            "(64, 48, 3)\n",
            "yup\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7319033-4670-42b2-852a-63c8b61fdb35",
        "outputId": "ee691e5d-5a66-4008-a184-e6c6ecebb74f"
      },
      "source": [
        "import sys\n",
        "print(sys.getsizeof(Data1))\n",
        "print(sys.getsizeof(y1))"
      ],
      "id": "a7319033-4670-42b2-852a-63c8b61fdb35",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "174182544\n",
            "151304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db9142c3-b7ac-4fdc-80f0-adb36a8147fa",
        "outputId": "bd926ebb-b070-45fd-d6fe-84947b3d537e"
      },
      "source": [
        "%load_ext memory_profiler\n",
        "%memit"
      ],
      "id": "db9142c3-b7ac-4fdc-80f0-adb36a8147fa",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "peak memory: 1749.21 MiB, increment: -0.69 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "38ec9b40-365d-479f-8137-6e125e242e9b",
        "outputId": "df5acb1f-fe59-4889-896b-a739d20f3827"
      },
      "source": [
        "%%time \n",
        "import _pickle, gzip, urllib.request, json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import np_utils\n",
        "\n",
        "train_set, valid_set, test_set = dataset\n",
        "    \n",
        "(train_images, train_labels), (valid_images, valid_labels), (test_images, test_labels) = train_set, valid_set, test_set\n",
        "\n",
        "train_images = train_images.reshape(train_images.shape[0], 64, 48, 3)\n",
        "valid_images = valid_images.reshape(valid_images.shape[0], 64, 48, 3)\n",
        "test_images = test_images.reshape(test_images.shape[0], 64, 48, 3)\n",
        "train_images = train_images.astype('float32')\n",
        "valid_images = valid_images.astype('float32')\n",
        "test_images = test_images.astype('float32')\n",
        "train_images /= 255\n",
        "valid_images /= 255\n",
        "test_images /= 255\n",
        "\n",
        "n_classes = 18\n",
        "print(\"Shape before one-hot encoding: \", train_labels.shape)\n",
        "train_labels = np_utils.to_categorical(train_labels, n_classes)\n",
        "valid_labels = np_utils.to_categorical(valid_labels, n_classes)\n",
        "test_labels = np_utils.to_categorical(test_labels, n_classes)\n",
        "print(\"Shape after one-hot encoding: \", train_labels.shape)\n",
        "\n",
        "'''\n",
        "for i in range(0,10000,2000):\n",
        "    img = train_set[0][i]\n",
        "    label = train_set[1][i]\n",
        "    img_reshape = img.reshape((64,48,3))\n",
        "    imgplot = plt.imshow(img_reshape)\n",
        "    print('This is a {}'.format(label))\n",
        "    plt.show()\n",
        "'''"
      ],
      "id": "38ec9b40-365d-479f-8137-6e125e242e9b",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape before one-hot encoding:  (17000,)\n",
            "Shape after one-hot encoding:  (17000, 18)\n",
            "CPU times: user 296 ms, sys: 208 ms, total: 504 ms\n",
            "Wall time: 502 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"\\nfor i in range(0,10000,2000):\\n    img = train_set[0][i]\\n    label = train_set[1][i]\\n    img_reshape = img.reshape((64,48,3))\\n    imgplot = plt.imshow(img_reshape)\\n    print('This is a {}'.format(label))\\n    plt.show()\\n\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08c6e274-92fc-4a09-9c83-2143131a34b3",
        "outputId": "16a4d63c-36f7-4000-e1c7-8f7855dbf45f"
      },
      "source": [
        "#es = callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "#save = tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
        "#callback = [es, save]\n",
        "\n",
        "#hist = model.fit(train_images, train_labels, batch_size=64, epochs=10, validation_data=(valid_images, valid_labels), callbacks=callback)\n",
        "\n",
        "model.fit(train_images, train_labels, batch_size=128, epochs=5, validation_data=(valid_images, valid_labels))\n",
        "print('fit end')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "ptinr('acc : ', test_acc)"
      ],
      "id": "08c6e274-92fc-4a09-9c83-2143131a34b3",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-08-26 06:17:30.071673: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "133/133 [==============================] - 380s 3s/step - loss: 1.8310 - accuracy: 0.4743 - val_loss: 4.9750 - val_accuracy: 0.0670\n",
            "Epoch 2/5\n",
            "133/133 [==============================] - 370s 3s/step - loss: 0.9394 - accuracy: 0.6897 - val_loss: 2.4830 - val_accuracy: 0.2900\n",
            "Epoch 3/5\n",
            "133/133 [==============================] - 373s 3s/step - loss: 0.5951 - accuracy: 0.7972 - val_loss: 2.6451 - val_accuracy: 0.3300\n",
            "Epoch 4/5\n",
            "133/133 [==============================] - 371s 3s/step - loss: 0.4625 - accuracy: 0.8447 - val_loss: 2.7761 - val_accuracy: 0.4730\n",
            "Epoch 5/5\n",
            "133/133 [==============================] - 369s 3s/step - loss: 0.3637 - accuracy: 0.8776 - val_loss: 2.5094 - val_accuracy: 0.4420\n",
            "fit end\n",
            "29/29 - 2s - loss: 1.7929 - accuracy: 0.5922\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'ptinr' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d4a9f53e5030>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mptinr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'acc : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ptinr' is not defined"
          ]
        }
      ]
    }
  ]
}

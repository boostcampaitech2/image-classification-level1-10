{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## ðŸ“ basic resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "built-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '/opt/ml/input/data/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a38035-ee89-4ed6-a0a0-3a23e33f64f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "def file_load(opt):\n",
    "    \n",
    "    #data_path = []\n",
    "    #f = open(\"{0}.txt\".format(opt), 'r')\n",
    "    files = glob.glob(\"/opt/ml/input/data/train/images/*/*\") \n",
    "    #while True:\n",
    "        #line = f.readline()\n",
    "        #if not line: break\n",
    "        #data_path.append(line[:-1])\n",
    "    #f.close()\n",
    "    return files\n",
    "\n",
    "class CustomDataset(data.Dataset):\n",
    "    def __init__(self, opt_data):\n",
    "        super(CustomDataset, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        opt_data : 'train', 'validation'\n",
    "        \n",
    "        \"\"\"\n",
    "        #self.file_list = file_load('/opt/ml/input/data/train/train_path.csv')\n",
    "        y = pd.read_csv('/opt/ml/input/data/train/train_path.csv', index_col=0)\n",
    "        self.y = y.values\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        x = np.load(self.file_list[index])\n",
    "        self.x_data = torch.from_numpy(x).float()\n",
    "        self.y_data = torch.from_numpy(self.y[index]).float()\n",
    "        return self.x_data, self.y_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    a = CustomDataset('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79db9a8d-01a6-408c-8ac0-a16e8699ff00",
   "metadata": {},
   "source": [
    "## call resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ba8a5a0-8ebf-4249-a53e-d1dd2e5e38fc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-26 12:17:22.363055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-26 12:17:22.364413: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-08-26 12:17:22.364488: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-08-26 12:17:22.364500: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-08-26 12:17:22.365325: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "model = ResNet50(include_top=True, weights=None, input_shape=(64, 48, 3), pooling=max, classes=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f5d6d9-a806-4926-b8d2-2bafa3a982d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.imagenet_utils import decode_predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e457ffb2-97ef-4aef-a79d-66dbe1007717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_gpus():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_visible_devices(gpus[0],'GPU')\n",
    "            tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1500)])\n",
    "        except RuntimeError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdc6f001-bf1a-42ad-b034-ae471794ae6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile end\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "print('compile end')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6401c-f150-4a21-97e4-4ba53624ddcc",
   "metadata": {},
   "source": [
    "## call image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54cde698-4db7-4a23-8e94-2a6b2e32a5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gonna process:\n",
      "\t /opt/ml/input/data/train/images/*/*\n",
      "\t 0 files processed\n",
      "\t 1000 files processed\n",
      "\t 2000 files processed\n",
      "\t 3000 files processed\n",
      "\t 4000 files processed\n",
      "\t 5000 files processed\n",
      "\t 6000 files processed\n",
      "\t 7000 files processed\n",
      "\t 8000 files processed\n",
      "\t 9000 files processed\n",
      "\t 10000 files processed\n",
      "\t 11000 files processed\n",
      "\t 12000 files processed\n",
      "\t 13000 files processed\n",
      "\t 14000 files processed\n",
      "\t 15000 files processed\n",
      "\t 16000 files processed\n",
      "\t 17000 files processed\n",
      "\t 18000 files processed\n",
      "done\n",
      "(64, 48, 3)\n",
      "Gonna process:\n",
      "\t /opt/ml/input/data/eval/images/*\n",
      "\t 0 files processed\n",
      "\t 1000 files processed\n",
      "\t 2000 files processed\n",
      "\t 3000 files processed\n",
      "\t 4000 files processed\n",
      "\t 5000 files processed\n",
      "\t 6000 files processed\n",
      "\t 7000 files processed\n",
      "\t 8000 files processed\n",
      "\t 9000 files processed\n",
      "\t 10000 files processed\n",
      "\t 11000 files processed\n",
      "\t 12000 files processed\n",
      "done\n",
      "(64, 48, 3)\n",
      "yup\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from numpy import genfromtxt\n",
    "import gzip\n",
    "import _pickle\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imageio\n",
    "import cv2\n",
    "\n",
    "\n",
    "def dir_to_dataset(glob_files, loc_train_labels=\"\"):\n",
    "    print(\"Gonna process:\\n\\t %s\"%glob_files)\n",
    "    dataset = []\n",
    "    for file_count, file_name in enumerate( sorted(glob(glob_files)) ):\n",
    "        img = imageio.imread(file_name, pilmode='RGB')\n",
    "        img = cv2.resize(img, (48, 64))\n",
    "        #pixels = [f[0] for f in list(img.getdata())]\n",
    "        dataset.append(img)\n",
    "        if file_count % 1000 == 0:\n",
    "            print(\"\\t %s files processed\"%file_count)\n",
    "    print('done')\n",
    "    print(img.shape)\n",
    "        \n",
    "    # outfile = glob_files+\"out\"\n",
    "    # np.save(outfile, dataset)\n",
    "    if len(loc_train_labels) > 0:\n",
    "        df = pd.read_csv(loc_train_labels, names = [\"class\"])\n",
    "        return np.array(dataset), np.array(df[\"class\"])\n",
    "    else:\n",
    "        return np.array(dataset)\n",
    "    \n",
    "Data1, y1 = dir_to_dataset(\"/opt/ml/input/data/train/images/*/*\",\"/opt/ml/input/data/train/train_path.csv\")\n",
    "Data2, y2 = dir_to_dataset(\"/opt/ml/input/data/eval/images/*\",\"/opt/ml/input/data/eval/info.csv\")\n",
    "\n",
    "print('yup')\n",
    "\n",
    "# Data and labels are read \n",
    "train_num = 17000\n",
    "valid_num = 1000\n",
    "test_num = 900\n",
    "submit_num  = 12600\n",
    "\n",
    "train_set_x = Data1[:train_num]\n",
    "train_set_y = y1[1:train_num+1]\n",
    "val_set_x = Data1[17000:17000+valid_num]\n",
    "val_set_y = y1[17001:17001+valid_num]\n",
    "test_set_x = Data1[18000:18000+test_num]\n",
    "test_set_y = y1[18001:18001+test_num]\n",
    "\n",
    "submit_set_x = Data2[:submit_num]\n",
    "submit_set_y = y1[1:submit_num+1]\n",
    "\n",
    "train_set = train_set_x, train_set_y\n",
    "val_set = val_set_x, val_set_y\n",
    "test_set = test_set_x, test_set_y\n",
    "\n",
    "submit_set = submit_set_x, submit_set_y\n",
    "\n",
    "dataset = [train_set, val_set, test_set, submit_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b8df6b-57a8-4962-bb49-6b97ff4aab80",
   "metadata": {},
   "source": [
    "## data setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38ec9b40-365d-479f-8137-6e125e242e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before one-hot encoding:  (17000,)\n",
      "Shape after one-hot encoding:  (17000, 18)\n",
      "CPU times: user 240 ms, sys: 300 ms, total: 540 ms\n",
      "Wall time: 542 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in range(0,10000,2000):\\n    img = train_set[0][i]\\n    label = train_set[1][i]\\n    img_reshape = img.reshape((64,48,3))\\n    imgplot = plt.imshow(img_reshape)\\n    print('This is a {}'.format(label))\\n    plt.show()\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "import _pickle, gzip, urllib.request, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "\n",
    "train_set, valid_set, test_set, submit_set = dataset\n",
    "    \n",
    "(train_images, train_labels), (valid_images, valid_labels), (test_images, test_labels), (submit_images, submit_labels) = train_set, valid_set, test_set, submit_set\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], 64, 48, 3)\n",
    "valid_images = valid_images.reshape(valid_images.shape[0], 64, 48, 3)\n",
    "test_images = test_images.reshape(test_images.shape[0], 64, 48, 3)\n",
    "submit_images = submit_images.reshape(submit_images.shape[0], 64, 48, 3)\n",
    "\n",
    "train_images = train_images.astype('float32')\n",
    "valid_images = valid_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "submit_images = submit_images.astype('float32')\n",
    "\n",
    "train_images /= 255\n",
    "valid_images /= 255\n",
    "test_images /= 255\n",
    "submit_images /= 255\n",
    "\n",
    "n_classes = 18\n",
    "print(\"Shape before one-hot encoding: \", train_labels.shape)\n",
    "train_labels = np_utils.to_categorical(train_labels, n_classes)\n",
    "valid_labels = np_utils.to_categorical(valid_labels, n_classes)\n",
    "test_labels = np_utils.to_categorical(test_labels, n_classes)\n",
    "print(\"Shape after one-hot encoding: \", train_labels.shape)\n",
    "\n",
    "'''\n",
    "for i in range(0,10000,2000):\n",
    "    img = train_set[0][i]\n",
    "    label = train_set[1][i]\n",
    "    img_reshape = img.reshape((64,48,3))\n",
    "    imgplot = plt.imshow(img_reshape)\n",
    "    print('This is a {}'.format(label))\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27093778-3435-43a2-888b-0495f42e3642",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08c6e274-92fc-4a09-9c83-2143131a34b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "133/133 [==============================] - 373s 3s/step - loss: 1.6886 - accuracy: 0.4982 - val_loss: 6.8036 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 372s 3s/step - loss: 0.8879 - accuracy: 0.7095 - val_loss: 3.0328 - val_accuracy: 0.4970\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 371s 3s/step - loss: 0.5980 - accuracy: 0.8046 - val_loss: 1.5788 - val_accuracy: 0.6920\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 372s 3s/step - loss: 0.4400 - accuracy: 0.8529 - val_loss: 1.8269 - val_accuracy: 0.6080\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 371s 3s/step - loss: 0.3183 - accuracy: 0.8938 - val_loss: 2.8904 - val_accuracy: 0.6390\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 370s 3s/step - loss: 0.2775 - accuracy: 0.9071 - val_loss: 2.8878 - val_accuracy: 0.5040\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 371s 3s/step - loss: 0.2301 - accuracy: 0.9216 - val_loss: 1.8387 - val_accuracy: 0.7000\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - 371s 3s/step - loss: 0.1728 - accuracy: 0.9421 - val_loss: 1.3821 - val_accuracy: 0.7230\n",
      "Epoch 9/10\n",
      "133/133 [==============================] - 370s 3s/step - loss: 0.1449 - accuracy: 0.9509 - val_loss: 2.1749 - val_accuracy: 0.6970\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - 370s 3s/step - loss: 0.1202 - accuracy: 0.9586 - val_loss: 1.3551 - val_accuracy: 0.7110\n",
      "fit end\n",
      "29/29 - 2s - loss: 1.8454 - accuracy: 0.5789\n",
      "acc :  0.5788888931274414\n"
     ]
    }
   ],
   "source": [
    "#es = callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "#save = tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "#callback = [es, save]\n",
    "\n",
    "#hist = model.fit(train_images, train_labels, batch_size=64, epochs=10, validation_data=(valid_images, valid_labels), callbacks=callback)\n",
    "\n",
    "model.fit(train_images, train_labels, batch_size=128, epochs=10, validation_data=(valid_images, valid_labels))\n",
    "print('fit end')\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('acc : ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a048474-078b-478c-bcc1-6e4d5e9c1bed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7eb5d11-8858-4a38-9c2b-64ecb5d91eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(submit_images)\n",
    "print(np.argmax(pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75410c0a-c762-468c-a3d3-2e1be03252d1",
   "metadata": {},
   "source": [
    "## make submit_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c588d2ae-3a59-4f3b-8c08-527d9110664b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import notebook\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "img_files = sorted(glob.glob(\"/opt/ml/input/data/eval/images/*\"))\n",
    "search = '/opt/ml/input/data/eval/images/'\n",
    "\n",
    "for i, word in enumerate(img_files):\n",
    "    if search in word: \n",
    "        img_files[i] = word.strip(search)\n",
    "\n",
    "#print(img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf26fd11-e16c-4f0e-8cc3-665f20ba0d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "submit_dic = {}\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    submit_dic[img_files[i]] = np.argmax(pred[i])\n",
    "\n",
    "#print(submit_dic[img_files[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe3f49d0-048c-425a-a028-e3ecd7715acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(submit_dic.items()),columns=['ImageID', 'ans'])\n",
    "df.to_csv('/opt/ml/input/data/eval/submit0826.csv', mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

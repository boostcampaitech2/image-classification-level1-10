{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c063ba-2390-4681-8caf-d508ca52fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from time import sleep\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e1350a-50f4-4e56-aa2f-27147e19472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04457ba3-fd7d-42ae-a462-6a45d3101c50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Model Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0538b2c-a9be-4aba-9cc4-d972f7cd1d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>stem</th>\n",
       "      <th>img_path</th>\n",
       "      <th>gender_issue</th>\n",
       "      <th>mask_issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>45</td>\n",
       "      <td>female</td>\n",
       "      <td>mask1</td>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>45</td>\n",
       "      <td>female</td>\n",
       "      <td>mask2</td>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000001</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>45</td>\n",
       "      <td>female</td>\n",
       "      <td>mask4</td>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>45</td>\n",
       "      <td>female</td>\n",
       "      <td>mask3</td>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000001</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>45</td>\n",
       "      <td>female</td>\n",
       "      <td>incorrect_mask</td>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                    path  age  gender            stem  \\\n",
       "0  000001  000001_female_Asian_45   45  female           mask1   \n",
       "1  000001  000001_female_Asian_45   45  female           mask2   \n",
       "2  000001  000001_female_Asian_45   45  female           mask4   \n",
       "3  000001  000001_female_Asian_45   45  female           mask3   \n",
       "4  000001  000001_female_Asian_45   45  female  incorrect_mask   \n",
       "\n",
       "                                            img_path  gender_issue  mask_issue  \n",
       "0  /opt/ml/input/data/train/images/000001_female_...         False       False  \n",
       "1  /opt/ml/input/data/train/images/000001_female_...         False       False  \n",
       "2  /opt/ml/input/data/train/images/000001_female_...         False       False  \n",
       "3  /opt/ml/input/data/train/images/000001_female_...         False       False  \n",
       "4  /opt/ml/input/data/train/images/000001_female_...         False       False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/opt/ml/code/data/Final.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77339a96-5cab-4620-9c96-83ade283aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = ''\n",
    "for i in range(len(df)) :\n",
    "    label = 0\n",
    "    # 나이를 기준으로 점수\n",
    "    if df['age'][i] < 30 :\n",
    "        label = 0\n",
    "    elif 30 <= df['age'][i] < 60 :\n",
    "        label = 1\n",
    "    elif 60 <= df['age'][i] :\n",
    "        label = 2\n",
    "    \n",
    "    # 여자일 경우 남자의 label +3\n",
    "    if df['gender'][i] == 'female' :\n",
    "        label += 3\n",
    "    \n",
    "    # mask상태가 Not wear일 경우 + 12, Incorrect일 경우 + 6\n",
    "    if df['stem'][i] == 'normal' :\n",
    "        label += 12\n",
    "    elif df['stem'][i] == 'incorrect_mask' :\n",
    "        label += 6\n",
    "    \n",
    "    df['label'][i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e66da0cd-f700-4712-b9ac-3e212b52baf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>stem</th>\n",
       "      <th>img_path</th>\n",
       "      <th>gender_issue</th>\n",
       "      <th>mask_issue</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>000041</td>\n",
       "      <td>000041_female_Asian_58</td>\n",
       "      <td>58</td>\n",
       "      <td>female</td>\n",
       "      <td>mask3</td>\n",
       "      <td>/opt/ml/input/data/train/images/000041_female_...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>000556</td>\n",
       "      <td>000556_male_Asian_57</td>\n",
       "      <td>57</td>\n",
       "      <td>male</td>\n",
       "      <td>mask5</td>\n",
       "      <td>/opt/ml/input/data/train/images/000556_male_As...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18697</th>\n",
       "      <td>006752</td>\n",
       "      <td>006752_male_Asian_19</td>\n",
       "      <td>19</td>\n",
       "      <td>male</td>\n",
       "      <td>mask1</td>\n",
       "      <td>/opt/ml/input/data/train/images/006752_male_As...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>003874</td>\n",
       "      <td>003874_male_Asian_60</td>\n",
       "      <td>60</td>\n",
       "      <td>male</td>\n",
       "      <td>incorrect_mask</td>\n",
       "      <td>/opt/ml/input/data/train/images/003874_male_As...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10755</th>\n",
       "      <td>003653</td>\n",
       "      <td>003653_male_Asian_54</td>\n",
       "      <td>54</td>\n",
       "      <td>male</td>\n",
       "      <td>mask3</td>\n",
       "      <td>/opt/ml/input/data/train/images/003653_male_As...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12707</th>\n",
       "      <td>004279</td>\n",
       "      <td>004279_female_Asian_60</td>\n",
       "      <td>60</td>\n",
       "      <td>female</td>\n",
       "      <td>mask4</td>\n",
       "      <td>/opt/ml/input/data/train/images/004279_female_...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8259</th>\n",
       "      <td>003135</td>\n",
       "      <td>003135_female_Asian_20</td>\n",
       "      <td>20</td>\n",
       "      <td>female</td>\n",
       "      <td>normal</td>\n",
       "      <td>/opt/ml/input/data/train/images/003135_female_...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10960</th>\n",
       "      <td>003719</td>\n",
       "      <td>003719_male_Asian_60</td>\n",
       "      <td>60</td>\n",
       "      <td>male</td>\n",
       "      <td>mask5</td>\n",
       "      <td>/opt/ml/input/data/train/images/003719_male_As...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15107</th>\n",
       "      <td>005443</td>\n",
       "      <td>005443_female_Asian_23</td>\n",
       "      <td>23</td>\n",
       "      <td>female</td>\n",
       "      <td>mask2</td>\n",
       "      <td>/opt/ml/input/data/train/images/005443_female_...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18391</th>\n",
       "      <td>006686</td>\n",
       "      <td>006686_male_Asian_20</td>\n",
       "      <td>20</td>\n",
       "      <td>male</td>\n",
       "      <td>mask4</td>\n",
       "      <td>/opt/ml/input/data/train/images/006686_male_As...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                    path  age  gender            stem  \\\n",
       "234    000041  000041_female_Asian_58   58  female           mask3   \n",
       "1223   000556    000556_male_Asian_57   57    male           mask5   \n",
       "18697  006752    006752_male_Asian_19   19    male           mask1   \n",
       "11820  003874    003874_male_Asian_60   60    male  incorrect_mask   \n",
       "10755  003653    003653_male_Asian_54   54    male           mask3   \n",
       "12707  004279  004279_female_Asian_60   60  female           mask4   \n",
       "8259   003135  003135_female_Asian_20   20  female          normal   \n",
       "10960  003719    003719_male_Asian_60   60    male           mask5   \n",
       "15107  005443  005443_female_Asian_23   23  female           mask2   \n",
       "18391  006686    006686_male_Asian_20   20    male           mask4   \n",
       "\n",
       "                                                img_path  gender_issue  \\\n",
       "234    /opt/ml/input/data/train/images/000041_female_...         False   \n",
       "1223   /opt/ml/input/data/train/images/000556_male_As...         False   \n",
       "18697  /opt/ml/input/data/train/images/006752_male_As...         False   \n",
       "11820  /opt/ml/input/data/train/images/003874_male_As...         False   \n",
       "10755  /opt/ml/input/data/train/images/003653_male_As...         False   \n",
       "12707  /opt/ml/input/data/train/images/004279_female_...         False   \n",
       "8259   /opt/ml/input/data/train/images/003135_female_...         False   \n",
       "10960  /opt/ml/input/data/train/images/003719_male_As...         False   \n",
       "15107  /opt/ml/input/data/train/images/005443_female_...         False   \n",
       "18391  /opt/ml/input/data/train/images/006686_male_As...         False   \n",
       "\n",
       "       mask_issue label  \n",
       "234         False     4  \n",
       "1223        False     1  \n",
       "18697       False     0  \n",
       "11820       False     8  \n",
       "10755       False     1  \n",
       "12707       False     5  \n",
       "8259        False    15  \n",
       "10960       False     2  \n",
       "15107       False     3  \n",
       "18391       False     0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c12b7437-98ce-42b9-8b23-62f821f0bd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (6,4.5))\n",
    "ax = sns.countplot(x = 'stem', data = df, palette = ['#55967e', '#264959', 'green'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afef51ec-1fae-4616-b711-bc77191fc40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18, 1344),\n",
       " (19, 2849),\n",
       " (20, 1869),\n",
       " (21, 287),\n",
       " (22, 406),\n",
       " (23, 504),\n",
       " (24, 434),\n",
       " (25, 574),\n",
       " (26, 308),\n",
       " (27, 105),\n",
       " (28, 175),\n",
       " (29, 112),\n",
       " (30, 119),\n",
       " (31, 21),\n",
       " (32, 49),\n",
       " (33, 35),\n",
       " (34, 91),\n",
       " (35, 133),\n",
       " (36, 56),\n",
       " (37, 21),\n",
       " (38, 98),\n",
       " (39, 42),\n",
       " (40, 140),\n",
       " (41, 49),\n",
       " (42, 91),\n",
       " (43, 126),\n",
       " (44, 91),\n",
       " (45, 189),\n",
       " (46, 98),\n",
       " (47, 70),\n",
       " (48, 273),\n",
       " (49, 217),\n",
       " (50, 560),\n",
       " (51, 441),\n",
       " (52, 546),\n",
       " (53, 448),\n",
       " (54, 574),\n",
       " (55, 700),\n",
       " (56, 861),\n",
       " (57, 742),\n",
       " (58, 1113),\n",
       " (59, 595),\n",
       " (60, 1344)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = dict(df['age'].value_counts())\n",
    "sorted(check.items(), key = lambda x : x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2c1c75-2131-443d-ba9b-08c10f11ea15",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fa6f249-16e4-40f7-a4bc-d03b29d11880",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDataset(Dataset) :\n",
    "    def __init__(self, path_list, label_list, age_list, gender_list, mask_list, transform, aug_transform = None, need = False) :\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.transform = transform\n",
    "        self.aug_transform = aug_transform\n",
    "        \n",
    "        for path, label, age, gender, mask in tqdm_notebook(zip(path_list, label_list, age_list, gender_list, mask_list)) :\n",
    "            image = Image.open(path)\n",
    "            self.X.append(image)\n",
    "            self.y.append(label)\n",
    "            if need :\n",
    "                if 21 <= age <= 26 and gender == 'male' and len(mask) == 5 :\n",
    "                    for _ in range(1) :\n",
    "                        self.y.append(label)\n",
    "                        self.X.append(Image.fromarray(self.aug_transform(image = np.array(image))['image']))\n",
    "                elif 21 <= age <= 26 and gender == 'male' :\n",
    "                    for _ in range(10) :\n",
    "                        self.y.append(label)\n",
    "                        self.X.append(Image.fromarray(self.aug_transform(image = np.array(image))['image']))\n",
    "                elif 21 <= age <= 26 and gender == 'female' and len(mask) == 5 :\n",
    "                    for _ in range(1) :\n",
    "                        self.y.append(label)\n",
    "                        self.X.append(Image.fromarray(self.aug_transform(image = np.array(image))['image']))\n",
    "                elif 21 <= age <= 26 and gender == 'female' :\n",
    "                    for _ in range(10) :\n",
    "                        self.y.append(label)\n",
    "                        self.X.append(Image.fromarray(self.aug_transform(image = np.array(image))['image']))\n",
    "                elif 27 <= age <= 47 and gender == 'male' and len(mask) == 5 :\n",
    "                    for _ in range(5) :\n",
    "                        self.y.append(label)\n",
    "                        self.X.append(Image.fromarray(self.aug_transform(image = np.array(image))['image']))\n",
    "                elif 27 <= age <= 47 and gender == 'male' :\n",
    "                    for _ in range(8) :\n",
    "                        self.y.append(label)\n",
    "                        self.X.append(Image.fromarray(self.aug_transform(image = np.array(image))['image']))\n",
    "                elif 27 <= age <= 47 and gender == 'female' and len(mask) == 5:\n",
    "                    for _ in range(5) :\n",
    "                        self.y.append(label)\n",
    "                        self.X.append(Image.fromarray(self.aug_transform(image = np.array(image))['image']))\n",
    "                elif 27 <= age <= 47 and gender == 'female' :\n",
    "                    for _ in range(8) :\n",
    "                        self.y.append(label)\n",
    "                        self.X.append(Image.fromarray(self.aug_transform(image = np.array(image))['image']))\n",
    "                elif 48 <= age <= 54 and gender == 'male' and len(mask) == 5 :\n",
    "                    for _ in range(2) :\n",
    "                        self.y.append(label)\n",
    "                        self.X.append(Image.fromarray(self.aug_transform(image = np.array(image))['image']))\n",
    "                elif 48 <= age <= 54 and gender == 'male' :\n",
    "                    for _ in range(2) :\n",
    "                        self.y.append(label)\n",
    "                        self.X.append(Image.fromarray(self.aug_transform(image = np.array(image))['image']))\n",
    "                elif 48 <= age <= 54 and gender == 'female' and len(mask) == 5:\n",
    "                    for _ in range(2) :\n",
    "                        self.y.append(label)\n",
    "                        self.X.append(Image.fromarray(self.aug_transform(image = np.array(image))['image']))\n",
    "                elif 48 <= age <= 54 and gender == 'female' :\n",
    "                    for _ in range(2) :\n",
    "                        self.y.append(label)\n",
    "                        self.X.append(Image.fromarray(self.aug_transform(image = np.array(image))['image']))\n",
    "    \n",
    "    def __getitem__(self, idx) :\n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        X = self.transform(image = np.array(X))['image']\n",
    "        return torch.tensor(X, dtype = torch.float), torch.tensor(y, dtype = torch.long)\n",
    "        \n",
    "    def __len__(self) :\n",
    "        len_dataset = len(self.X)\n",
    "        return len_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "976207a0-b2b6-45b6-8560-688a42c27d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = df['img_path']\n",
    "target = np.array(df['label'])\n",
    "age_list = np.array(df['age'])\n",
    "gender_list = np.array(df['gender'])\n",
    "mask_list = np.array(df['stem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42dfa3-5076-43b4-b068-5d74d66fed0e",
   "metadata": {},
   "source": [
    "### ResNext50_32x4d With Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7cd9efc-3b3a-4234-97b5-1d6c45f3f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnext50_32x4d(pretrained=True)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1024, bias = True),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Dropout(p = 0.3),\n",
    "    nn.Linear(1024, 256, bias = True),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Dropout(p = 0.3),\n",
    "    nn.Linear(256, 18, bias = True))\n",
    "\n",
    "torch.nn.init.xavier_uniform_(model.fc[0].weight)\n",
    "stdv = 1.0 / np.sqrt(model.fc[0].in_features)\n",
    "model.fc[0].bias.data.uniform_(-stdv,stdv)\n",
    "\n",
    "for param in model.parameters() : # frozon\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "797a7ae1-40e0-42ef-96a7-db5dd61bcb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6b800acb0140368f12af5057a1724b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c629e12699542d097774732948b7e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cuda:0 is using !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [11:42<00:00,  1.34s/it]\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 train-데이터 셋에서 평균 Loss : 0.5421, 평균 Accuracy : 0.8312, 평균 F1 Score :  0.7187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:57<00:00,  1.29it/s]\n",
      "  0%|          | 0/526 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 test-데이터 셋에서 평균 Loss : 0.1503, 평균 Accuracy : 0.9498, 평균 F1 Score :  0.8763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [11:16<00:00,  1.29s/it]\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-1의 train-데이터 셋에서 평균 Loss : 0.0938, 평균 Accuracy : 0.9693, 평균 F1 Score :  0.9275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:43<00:00,  1.71it/s]\n",
      "  0%|          | 0/526 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-1의 test-데이터 셋에서 평균 Loss : 0.0675, 평균 Accuracy : 0.9790, 평균 F1 Score :  0.9407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [11:18<00:00,  1.29s/it]\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-2의 train-데이터 셋에서 평균 Loss : 0.0565, 평균 Accuracy : 0.9825, 평균 F1 Score :  0.9624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:43<00:00,  1.71it/s]\n",
      "  0%|          | 0/526 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-2의 test-데이터 셋에서 평균 Loss : 0.0672, 평균 Accuracy : 0.9752, 평균 F1 Score :  0.9308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [11:17<00:00,  1.29s/it]\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-3의 train-데이터 셋에서 평균 Loss : 0.0433, 평균 Accuracy : 0.9858, 평균 F1 Score :  0.9703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:43<00:00,  1.72it/s]\n",
      "  0%|          | 0/526 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-3의 test-데이터 셋에서 평균 Loss : 0.0418, 평균 Accuracy : 0.9873, 평균 F1 Score :  0.9729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [11:17<00:00,  1.29s/it]\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-4의 train-데이터 셋에서 평균 Loss : 0.0296, 평균 Accuracy : 0.9905, 평균 F1 Score :  0.9811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:43<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-4의 test-데이터 셋에서 평균 Loss : 0.0432, 평균 Accuracy : 0.9877, 평균 F1 Score :  0.9681\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6944c305f4b4c3c9e188c9f6ef6c992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c0a8855f0c4f55a2f0858715543061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cuda:0 is using !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [11:24<00:00,  1.29s/it]\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 train-데이터 셋에서 평균 Loss : 0.0590, 평균 Accuracy : 0.9814, 평균 F1 Score :  0.9667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:53<00:00,  1.39it/s]\n",
      "  0%|          | 0/530 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 test-데이터 셋에서 평균 Loss : 0.0091, 평균 Accuracy : 0.9972, 평균 F1 Score :  0.9956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [10:59<00:00,  1.24s/it]\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-1의 train-데이터 셋에서 평균 Loss : 0.0327, 평균 Accuracy : 0.9894, 평균 F1 Score :  0.9793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:39<00:00,  1.86it/s]\n",
      "  0%|          | 0/530 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-1의 test-데이터 셋에서 평균 Loss : 0.0289, 평균 Accuracy : 0.9894, 평균 F1 Score :  0.9616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [10:59<00:00,  1.24s/it]\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-2의 train-데이터 셋에서 평균 Loss : 0.0298, 평균 Accuracy : 0.9908, 평균 F1 Score :  0.9819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:39<00:00,  1.87it/s]\n",
      "  0%|          | 0/530 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-2의 test-데이터 셋에서 평균 Loss : 0.0325, 평균 Accuracy : 0.9886, 평균 F1 Score :  0.9739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [10:58<00:00,  1.24s/it]\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-3의 train-데이터 셋에서 평균 Loss : 0.0266, 평균 Accuracy : 0.9919, 평균 F1 Score :  0.9828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:39<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping!\n",
      "fold2 Stopped\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30fa6c0de9864956b962a401b5dfd1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c484ed3de714f8bacd34a5de47b6528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cuda:0 is using !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 532/532 [11:34<00:00,  1.31s/it]\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 train-데이터 셋에서 평균 Loss : 0.0366, 평균 Accuracy : 0.9882, 평균 F1 Score :  0.9789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:54<00:00,  1.36it/s]\n",
      "  0%|          | 0/532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 test-데이터 셋에서 평균 Loss : 0.0021, 평균 Accuracy : 1.0000, 평균 F1 Score :  1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 532/532 [11:08<00:00,  1.26s/it]\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-1의 train-데이터 셋에서 평균 Loss : 0.0244, 평균 Accuracy : 0.9918, 평균 F1 Score :  0.9853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:41<00:00,  1.80it/s]\n",
      "  0%|          | 0/532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-1의 test-데이터 셋에서 평균 Loss : 0.0118, 평균 Accuracy : 0.9958, 평균 F1 Score :  0.9931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 532/532 [11:08<00:00,  1.26s/it]\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-2의 train-데이터 셋에서 평균 Loss : 0.0265, 평균 Accuracy : 0.9917, 평균 F1 Score :  0.9853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:41<00:00,  1.80it/s]\n",
      "  0%|          | 0/532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-2의 test-데이터 셋에서 평균 Loss : 0.0316, 평균 Accuracy : 0.9937, 평균 F1 Score :  0.9867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 532/532 [11:08<00:00,  1.26s/it]\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-3의 train-데이터 셋에서 평균 Loss : 0.0237, 평균 Accuracy : 0.9928, 평균 F1 Score :  0.9874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:41<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping!\n",
      "fold3 Stopped\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370bad2dab584bdf9c141cab8fa34a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a38003fdf94c9aa9d7318063765454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cuda:0 is using !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 532/532 [11:27<00:00,  1.29s/it]\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 train-데이터 셋에서 평균 Loss : 0.0290, 평균 Accuracy : 0.9906, 평균 F1 Score :  0.9837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:53<00:00,  1.39it/s]\n",
      "  0%|          | 0/532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 test-데이터 셋에서 평균 Loss : 0.0029, 평균 Accuracy : 0.9989, 평균 F1 Score :  0.9981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 532/532 [10:59<00:00,  1.24s/it]\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-1의 train-데이터 셋에서 평균 Loss : 0.0228, 평균 Accuracy : 0.9927, 평균 F1 Score :  0.9873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:39<00:00,  1.85it/s]\n",
      "  0%|          | 0/532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-1의 test-데이터 셋에서 평균 Loss : 0.0033, 평균 Accuracy : 0.9987, 평균 F1 Score :  0.9966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 532/532 [11:00<00:00,  1.24s/it]\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-2의 train-데이터 셋에서 평균 Loss : 0.0201, 평균 Accuracy : 0.9935, 평균 F1 Score :  0.9879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:39<00:00,  1.88it/s]\n",
      "  0%|          | 0/532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-2의 test-데이터 셋에서 평균 Loss : 0.0470, 평균 Accuracy : 0.9879, 평균 F1 Score :  0.9732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 532/532 [10:59<00:00,  1.24s/it]\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-3의 train-데이터 셋에서 평균 Loss : 0.0172, 평균 Accuracy : 0.9949, 평균 F1 Score :  0.9908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:39<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping!\n",
      "fold4 Stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "stf = StratifiedKFold(n_splits = 4, shuffle = True, random_state =42)\n",
    "foldperf = {}\n",
    "for fold, (train_idx, valid_idx) in enumerate(stf.split(data_path, list(target))) :\n",
    "    \n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    target_array = np.array(target)\n",
    "    \n",
    "    dataset_train_Mask = MaskDataset(path_list = data_path[train_idx],\n",
    "                                     label_list = target[train_idx],\n",
    "                                     age_list = age_list[train_idx],\n",
    "                                     gender_list = gender_list[train_idx],\n",
    "                                     mask_list = mask_list[train_idx],\n",
    "                                     transform = Compose([\n",
    "                                             Resize(512, 384, p = 1.0),\n",
    "                                             Normalize(mean = (0.5,0.5,0.5), std = (0.2, 0.2, 0.2), max_pixel_value = 255.0, p = 1.0),\n",
    "                                             ToTensorV2(p = 1.0),\n",
    "                                     ]),\n",
    "                                     aug_transform = Compose([\n",
    "                                         Resize(512, 384, p = 1.0),\n",
    "                                         HorizontalFlip(p = 0.5),\n",
    "                                         RandomBrightnessContrast(brightness_limit = (-0.3, 0.3), contrast_limit = (-0.3, 0.3), p = 1.0),\n",
    "                                         GaussNoise(var_limit = (1000, 1600), p = 1.0),\n",
    "                                         CLAHE(p = 1.0),\n",
    "                                         Equalize(p = 1.0),\n",
    "                                         ShiftScaleRotate(p = 1.0),\n",
    "                                      ]),\n",
    "                                     need = True\n",
    "                                      )\n",
    "    dataset_valid_Mask = MaskDataset(path_list = data_path[valid_idx],\n",
    "                                     label_list = target[valid_idx],\n",
    "                                     age_list = age_list[valid_idx],\n",
    "                                     gender_list = gender_list[valid_idx],\n",
    "                                     mask_list = mask_list[valid_idx],\n",
    "                                     transform = Compose([\n",
    "                                             Resize(512, 384, p = 1.0),\n",
    "                                             Normalize(mean = (0.5,0.5,0.5), std = (0.2, 0.2, 0.2), max_pixel_value = 255.0, p = 1.0),\n",
    "                                             ToTensorV2(p = 1.0),\n",
    "                                     ]))\n",
    "    BATCH_SIZE = 64\n",
    "    mask_train_dataloader = torch.utils.data.DataLoader(dataset_train_Mask,\n",
    "                                                        batch_size = BATCH_SIZE,\n",
    "                                                        shuffle = True)\n",
    "    mask_valid_dataloader = torch.utils.data.DataLoader(dataset_valid_Mask,\n",
    "                                                        batch_size = BATCH_SIZE,\n",
    "                                                        shuffle = True)\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'{device} is using !')\n",
    "    sleep(1)\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    LEARNING_RATE = 0.0001\n",
    "    NUM_EPOCH = 100\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "    dataloaders = {\n",
    "        'train' : mask_train_dataloader,\n",
    "        'test' : mask_valid_dataloader\n",
    "    }\n",
    "    \n",
    "    history = {'train_loss' : [], 'test_loss' : [],\n",
    "               'train_acc' : [], 'test_acc' : [],\n",
    "               'train_f1' : [], 'test_f1' : []}\n",
    "    \n",
    "    n_epochs_stop = 3\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    min_val_loss = np.Inf\n",
    "    \n",
    "    best_test_accuracy = 0\n",
    "    best_test_loss = 9999.\n",
    "    for epoch in range(5) :\n",
    "        for phase in ['train', 'test'] :\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "            running_f1 = 0.\n",
    "            n_iter = 0\n",
    "            \n",
    "            if phase == 'train' :\n",
    "                model.train()\n",
    "            elif phase == 'test' :\n",
    "                model.eval()\n",
    "\n",
    "            for ind, (images, labels) in enumerate(tqdm(dataloaders[phase])) :\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train') : # phase == 'train'일 경우에만 grad_enabled를 True\n",
    "                    logits = model(images)\n",
    "                    _, preds = torch.max(logits, 1)\n",
    "                    loss = loss_fn(logits, labels)\n",
    "                    \n",
    "                    if phase == 'train' :\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                running_acc += torch.sum(preds == labels.data)\n",
    "                running_f1 += f1_score(preds.cpu().numpy(), labels.cpu().numpy(), average = 'macro')\n",
    "                n_iter += 1\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_acc / len(dataloaders[phase].dataset)\n",
    "            epoch_f1 = running_f1 / n_iter\n",
    "            #epoch_f1_check = running_f1 / len(dataloaders[phase].dataset)\n",
    "            #print('n_iter는 ', n_iter, ' dataset는 :', len(dataloaders[phase].dataset))\n",
    "            #print('n_iter :', epoch_f1, ' dataset :', epoch_f1_check)\n",
    "            \n",
    "            if phase == 'test' :\n",
    "                if epoch_loss < min_val_loss :\n",
    "                    epochs_no_improve = 0\n",
    "                    min_val_loss = epoch_loss\n",
    "                else :\n",
    "                    epochs_no_improve += 1\n",
    "                \n",
    "                if epochs_no_improve == n_epochs_stop :\n",
    "                    print('Early Stopping!')\n",
    "                    early_stop = True\n",
    "                    break\n",
    "            \n",
    "            if phase == 'train' :\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "                history['train_acc'].append(epoch_acc)\n",
    "                history['train_f1'].append(epoch_f1)\n",
    "            elif phase == 'test' :\n",
    "                history['test_loss'].append(epoch_loss)\n",
    "                history['test_acc'].append(epoch_acc)\n",
    "                history['test_f1'].append(epoch_f1)\n",
    "\n",
    "            print(f\"현재 epoch-{epoch}의 {phase}-데이터 셋에서 평균 Loss : {epoch_loss:.4f}, 평균 Accuracy : {epoch_acc:.4f}, 평균 F1 Score : {epoch_f1: .4f}\")\n",
    "            #if phase == \"test\" and best_test_accuracy < epoch_acc: # phase가 test일 때, best accuracy 계산\n",
    "            #    best_test_accuracy = epoch_acc\n",
    "            #if phase == \"test\" and best_test_loss > epoch_loss: # phase가 test일 때, best loss 계산\n",
    "            #    best_test_loss = epoch_loss\n",
    "    \n",
    "        if early_stop :\n",
    "            print(f'fold{fold+1} Stopped')\n",
    "            break\n",
    "    foldperf['fold{}'.format(fold+1)] = history \n",
    "    \n",
    "    #print(\"학습 종료!\")\n",
    "    #print(f\"최고 accuracy : {best_test_accuracy}, 최고 낮은 loss : {best_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f37652a5-0ccc-421e-8dd6-b61a88adfa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/opt/ml/code/model/Resnext50_SKFold4-5-earlystopping-Augmentation.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b06ea17b-24c1-46bd-9b66-d80c18609f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbe1b133-51f4-4bc7-b149-6d188ae6c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad8b0364-c5e3-4972-8673-abd009e301bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c325274b-a74b-4ee6-84d2-bfc5ab21c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, utils\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffba0c53-a99f-4a6a-accc-5cf5d3f23f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=256, out_features=18, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "model = torch.load('/opt/ml/code/model/Resnext50_SKFold4-5-earlystopping-Augmentation.pt').to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f3c004f-7637-439c-8083-7fe40a9665d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12600/12600 [03:52<00:00, 54.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in tqdm(loader):\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a94cce20-7406-4126-8d41-ca75c410e291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission_tenth.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2e7d3d7-b5eb-4f92-8b9a-dcdeefb6cae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34044"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train_Mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c305f61-ad6a-4710-83b6-b96b9408551a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34044/34044 [01:45<00:00, 322.00it/s]\n"
     ]
    }
   ],
   "source": [
    "new_y = []\n",
    "for i in tqdm(range(len(dataset_train_Mask))) :\n",
    "    new_y.append(dataset_train_Mask[i][1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c51600f-c9f6-484d-be86-d256c54bc613",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame(data = new_y, columns = ['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62557fbf-bfd9-42ab-a60a-68f91e72e92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "4        8170\n",
       "3        3930\n",
       "1        3836\n",
       "0        3238\n",
       "6        2104\n",
       "9        2038\n",
       "15       2027\n",
       "16       1992\n",
       "10       1971\n",
       "12       1951\n",
       "7         894\n",
       "13        885\n",
       "5         409\n",
       "2         312\n",
       "11         82\n",
       "17         81\n",
       "14         62\n",
       "8          62\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbe44df-a170-4184-adf8-d438e5f1f262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5648b9fe-7368-42fd-830b-54e377836a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d4994-79cf-4840-9718-e191221f921f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e54785e6-f53a-4f74-9a12-231423166a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(data_path,\n",
    "                                                      target,\n",
    "                                                      test_size = 0.2,\n",
    "                                                      shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21f60815-e342-426f-9ef8-c07d73088b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx_list = list(X_train.index)\n",
    "valid_idx_list = list(X_valid.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ec85fbd-ee89-4e77-9ed7-3cc7a1f5df91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b90d18e0f54402ae5c2d63a71cf091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e49fe7112ba4c398d5026490f410d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "dataset_train_Mask = MaskDataset(path_list = data_path[train_idx_list],\n",
    "                                 label_list = target[train_idx_list],\n",
    "                                 age_list = age_list[train_idx_list],\n",
    "                                 gender_list = gender_list[train_idx_list],\n",
    "                                 mask_list = mask_list[train_idx_list],\n",
    "                                 transform = Compose([\n",
    "                                         Resize(512, 384, p = 1.0),\n",
    "                                         Normalize(mean = (0.5,0.5,0.5), std = (0.2, 0.2, 0.2), max_pixel_value = 255.0, p = 1.0),\n",
    "                                         ToTensorV2(p = 1.0),\n",
    "                                 ]),\n",
    "                                 aug_transform = Compose([\n",
    "                                     Resize(512, 384, p = 1.0),\n",
    "                                     HorizontalFlip(p = 0.5),\n",
    "                                     RandomBrightnessContrast(brightness_limit = (-0.3, 0.3), contrast_limit = (-0.3, 0.3), p = 1.0),\n",
    "                                     GaussNoise(var_limit = (1000, 1600), p = 1.0),\n",
    "                                     CLAHE(p = 1.0),\n",
    "                                     Equalize(p = 1.0),\n",
    "                                     ShiftScaleRotate(p = 1.0),\n",
    "                                  ]),\n",
    "                                 need = True\n",
    "                                  )\n",
    "dataset_valid_Mask = MaskDataset(path_list = data_path[valid_idx_list],\n",
    "                                 label_list = target[valid_idx_list],\n",
    "                                 age_list = age_list[valid_idx_list],\n",
    "                                 gender_list = gender_list[valid_idx_list],\n",
    "                                 mask_list = mask_list[valid_idx_list],\n",
    "                                 transform = Compose([\n",
    "                                         Resize(512, 384, p = 1.0),\n",
    "                                         Normalize(mean = (0.5,0.5,0.5), std = (0.2, 0.2, 0.2), max_pixel_value = 255.0, p = 1.0),\n",
    "                                         ToTensorV2(p = 1.0),\n",
    "                                 ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b31d125-1f41-46db-ab52-98439405132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "mask_train_dataloader = torch.utils.data.DataLoader(dataset_train_Mask,\n",
    "                                                    batch_size = BATCH_SIZE,\n",
    "                                                    shuffle = True)\n",
    "mask_valid_dataloader = torch.utils.data.DataLoader(dataset_valid_Mask,\n",
    "                                                    batch_size = BATCH_SIZE,\n",
    "                                                    shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13c234c5-6c85-40ef-8ab5-3c6a3044a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnext50_32x4d(pretrained=True)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1024, bias = True),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Dropout(p = 0.3),\n",
    "    nn.Linear(1024, 256, bias = True),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Dropout(p = 0.3),\n",
    "    nn.Linear(256, 18, bias = True))\n",
    "\n",
    "torch.nn.init.xavier_uniform_(model.fc[0].weight)\n",
    "stdv = 1.0 / np.sqrt(model.fc[0].in_features)\n",
    "model.fc[0].bias.data.uniform_(-stdv,stdv)\n",
    "\n",
    "for param in model.parameters() : # frozon\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "644fb7f6-05f8-4058-8c6c-261cceab2490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 is using !\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device} is using !')\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCH = 100\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "dataloaders = {\n",
    "    'train' : mask_train_dataloader,\n",
    "    'test' : mask_valid_dataloader\n",
    "}\n",
    "\n",
    "n_epochs_stop = 3\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "min_val_loss = np.Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50d2fcc9-cfb4-43f4-84e4-de57047d0cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 331/331 [07:33<00:00,  1.37s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 train-데이터 셋에서 평균 Loss : 0.6915, 평균 Accuracy : 0.7888, 평균 F1 Score :  0.6107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:45<00:00,  1.32it/s]\n",
      "  0%|          | 0/331 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 test-데이터 셋에서 평균 Loss : 0.2078, 평균 Accuracy : 0.9325, 평균 F1 Score :  0.8160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/331 [00:02<12:47,  2.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-9f426743868a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_test_accuracy = 0\n",
    "best_test_loss = 9999.\n",
    "for epoch in range(100) :\n",
    "    for phase in ['train', 'test'] :\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        running_f1 = 0.\n",
    "        n_iter = 0\n",
    "\n",
    "        if phase == 'train' :\n",
    "            model.train()\n",
    "        elif phase == 'test' :\n",
    "            model.eval()\n",
    "\n",
    "        for ind, (images, labels) in enumerate(tqdm(dataloaders[phase])) :\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train') : # phase == 'train'일 경우에만 grad_enabled를 True\n",
    "                logits = model(images)\n",
    "                _, preds = torch.max(logits, 1)\n",
    "                loss = loss_fn(logits, labels)\n",
    "\n",
    "                if phase == 'train' :\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_acc += torch.sum(preds == labels.data)\n",
    "            running_f1 += f1_score(preds.cpu().numpy(), labels.cpu().numpy(), average = 'macro')\n",
    "            n_iter += 1\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        epoch_acc = running_acc / len(dataloaders[phase].dataset)\n",
    "        epoch_f1 = running_f1 / n_iter\n",
    "\n",
    "        if phase == 'test' :\n",
    "            if epoch_loss < min_val_loss :\n",
    "                epochs_no_improve = 0\n",
    "                min_val_loss = epoch_loss\n",
    "            else :\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve == n_epochs_stop :\n",
    "                print('Early Stopping!')\n",
    "                early_stop = True\n",
    "                break\n",
    "\n",
    "        print(f\"현재 epoch-{epoch}의 {phase}-데이터 셋에서 평균 Loss : {epoch_loss:.4f}, 평균 Accuracy : {epoch_acc:.4f}, 평균 F1 Score : {epoch_f1: .4f}\")\n",
    "        if phase == \"test\" and best_test_accuracy < epoch_acc: # phase가 test일 때, best accuracy 계산\n",
    "            best_test_accuracy = epoch_acc\n",
    "        if phase == \"test\" and best_test_loss > epoch_loss: # phase가 test일 때, best loss 계산\n",
    "            best_test_loss = epoch_loss\n",
    "\n",
    "    if early_stop :\n",
    "        break\n",
    "\n",
    "print(\"학습 종료!\")\n",
    "print(f\"최고 accuracy : {best_test_accuracy}, 최고 낮은 loss : {best_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c246d42-35c3-4db6-9227-b12b55d299ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
